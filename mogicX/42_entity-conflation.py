# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/42_entity-conflation.ipynb.

# %% auto 0
__all__ = ['show_conflated_labels', 'load_data', 'Filter', 'get_one_hop', 'normalize_matrix', 'compute_embed_similarity',
           'get_components', 'get_valid_components', 'get_conflated_info', 'get_id_to_cluster_idx_mapping',
           'get_conflated_matrix', 'cluster_length_stats', 'get_conflated_path', 'save_conflated_data', 'main',
           'parse_args']

# %% ../nbs/42_entity-conflation.ipynb 2
import scipy.sparse as sp, numpy as np, argparse, os, torch, pandas as pd
from tqdm.auto import tqdm
from termcolor import colored, COLORS
from scipy.sparse.csgraph import connected_components
from typing import List, Optional, Dict, Set, Tuple

from sugar.core import load_raw_file, save_raw_file
from xclib.utils.sparse import retain_topk

# %% ../nbs/42_entity-conflation.ipynb 5
def show_conflated_labels(idxs:List, components:Dict, lbl_ids2txt:Dict, fname:Optional[str]=None):
    file = fname if fname is None else open(fname, 'w')
    for i, idx in enumerate(idxs):
        txt = " || ".join([lbl_ids2txt[o] for o in components[idx]])
        if fname is None: print(f'{i+1:03d}. {txt}')
        else: file.write(f'{i+1:03d}. {txt}\n')
    if fname is not None: file.close()
        

# %% ../nbs/42_entity-conflation.ipynb 9
def load_data(pred_file:str, trn_file:str, tst_file:str, lbl_file:str, embed_file:Optional[str]=None,
              encoding:Optional[str]='utf-8'):
    pred_lbl, trn_lbl, tst_lbl = sp.load_npz(pred_file), sp.load_npz(trn_file), sp.load_npz(tst_file)
    lbl_ids, lbl_txt = load_raw_file(lbl_file, encoding=encoding)
    lbl_repr = None if embed_file is None else torch.load(embed_file)
    return pred_lbl, trn_lbl, tst_lbl, (lbl_ids, lbl_txt), lbl_repr
    

# %% ../nbs/42_entity-conflation.ipynb 15
class Filter:

    @staticmethod
    def by_length(components:Dict, min_thresh:Optional[int]=1, max_thresh:Optional[int]=100):
        cluster_len = np.array([len(components[idx]) for idx in sorted(components)])
        mask = np.logical_and(np.where(cluster_len >= min_thresh, 1, 0), np.where(cluster_len <= max_thresh, 1, 0))
        return set(np.where(mask)[0])

    @staticmethod
    def topk(data_lbl:sp.csr_matrix, k:Optional[int]=3):
        return retain_topk(data_lbl, k=k)

    @staticmethod
    def threshold(data_lbl:sp.csr_matrix, t:int):
        idx = np.where(data_lbl.data < t)[0]
        data_lbl.data[idx] = 0
        data_lbl.eliminate_zeros()
        return data_lbl

    @staticmethod
    def difference(data_lbl:sp.csr_matrix, t:int):
        rowise_max = data_lbl.max(axis=1).toarray().ravel()
        scores = np.repeat(rowise_max, np.diff(data_lbl.indptr)) - data_lbl.data
        data_lbl.data[scores > t] = 0
        data_lbl.eliminate_zeros()
        return data_lbl
        

# %% ../nbs/42_entity-conflation.ipynb 17
def get_one_hop(data_lbl:sp.csr_matrix, batch_size:Optional[int]=1024):
    data_lbl = data_lbl.copy()
    data_lbl.data[:] = 1.0
    
    lbl_data = data_lbl.T.tocsr()
    lbl_lbl = [lbl_data[i:i+batch_size]@data_lbl for i in tqdm(range(0, lbl_data.shape[0], batch_size))]
    return sp.vstack(lbl_lbl)
    

# %% ../nbs/42_entity-conflation.ipynb 19
def normalize_matrix(lbl_lbl:sp.csr_matrix):
    lbl_lbl = lbl_lbl + sp.eye(lbl_lbl.shape[0])
    row_deg, col_deg = lbl_lbl.sum(axis=1), lbl_lbl.sum(axis=0)
    lbl_lbl = lbl_lbl.multiply(np.sqrt(1/row_deg)).multiply(np.sqrt(1/col_deg)).tocsr()
    return lbl_lbl
    

# %% ../nbs/42_entity-conflation.ipynb 20
def compute_embed_similarity(lbl_lbl:sp.csr_matrix, lbl_repr:torch.Tensor, batch_size:Optional[int]=1024):
    lbl_lbl = lbl_lbl.tocoo()
    scores = []
    for i in tqdm(range(0, lbl_lbl.nnz, batch_size)):
        row_idx, col_idx = lbl_lbl.row[i:i+batch_size], lbl_lbl.col[i:i+batch_size]
        sc = lbl_repr[row_idx].view(len(row_idx), 1, -1) @ lbl_repr[col_idx].view(len(col_idx), -1, 1)
        scores.append(sc.squeeze(1).squeeze(1))
    scores = torch.hstack(scores)
    lbl_lbl.data[:] = scores.numpy()
    return lbl_lbl.tocsr()
    

# %% ../nbs/42_entity-conflation.ipynb 21
def get_components(data_lbl:sp.csr_matrix, lbl_ids:List, lbl_repr:Optional[torch.Tensor]=None, 
                   score_thresh:Optional[float]=25, freq_thresh:Optional[float]=50, batch_size:Optional[int]=1024):
    lbl_lbl = get_one_hop(data_lbl, batch_size)
    lbl_lbl = normalize_matrix(lbl_lbl)
    lbl_lbl = Filter.threshold(lbl_lbl, t=np.percentile(lbl_lbl.data, q=freq_thresh))
    
    if lbl_repr is not None:
        lbl_lbl = compute_embed_similarity(lbl_lbl, lbl_repr, batch_size=batch_size)
        lbl_lbl = Filter.threshold(lbl_lbl, t=np.percentile(lbl_lbl.data, q=score_thresh))
    
    n_comp, clusters = connected_components(lbl_lbl, directed=False, return_labels=True)
    components = {}
    for idx,ids in zip(clusters, lbl_ids):
        components.setdefault(idx, []).append(ids)
    return components
    

# %% ../nbs/42_entity-conflation.ipynb 29
def get_valid_components(components:Dict, valid_cluster_idxs:Set):
    valid_components, lbl_ids2cluster = dict(), dict()

    curr_cluster_idx = 0
    for idx, cluster in components.items():
        if idx in valid_cluster_idxs:
            valid_components[curr_cluster_idx] = cluster
            for o in cluster: lbl_ids2cluster[o] = curr_cluster_idx
            curr_cluster_idx += 1
        else:
            for o in cluster:
                valid_components[curr_cluster_idx] = [o]
                lbl_ids2cluster[o] = curr_cluster_idx
                curr_cluster_idx += 1
                
    return valid_components, lbl_ids2cluster
    

# %% ../nbs/42_entity-conflation.ipynb 30
def get_conflated_info(components:Dict, lbl_ids2txt:Dict):
    return [" || ".join([lbl_ids2txt[o] for o in components[i]]) for i in sorted(components)]
        

# %% ../nbs/42_entity-conflation.ipynb 31
def get_id_to_cluster_idx_mapping(lbl_ids2cluster_map:Dict, lbl_ids:List):
    return np.array([lbl_ids2cluster_map[o] for o in lbl_ids])
    

# %% ../nbs/42_entity-conflation.ipynb 32
def get_conflated_matrix(data_lbl:sp.csr_matrix, lbl_ids2cluster:Dict, n_clusters:Optional[Tuple]=None):
    indices = [lbl_ids2cluster[idx] for idx in data_lbl.indices]
    data = len(indices) * [1]
    
    matrix = (
        sp.csr_matrix((data, indices, data_lbl.indptr), dtype=np.float32) 
        if n_clusters is None else 
        sp.csr_matrix((data, indices, data_lbl.indptr), shape=(data_lbl.shape[0], n_clusters), dtype=np.float32)
    )
    matrix.sum_duplicates()
    return matrix
    

# %% ../nbs/42_entity-conflation.ipynb 36
def cluster_length_stats(components):
    print(f'Number of components: {len(components)}')
    lengths = np.array([len(o) for o in valid_components.values() if len(o) > 1])
    print(f'Number of clusters: {len(lengths)}', end='\n\n')
    with pd.option_context('display.precision', 3):
        print(pd.DataFrame(lengths).describe().T)
        

# %% ../nbs/42_entity-conflation.ipynb 42
def get_conflated_path(fname):
    file_dir = os.path.dirname(fname)
    file_name, file_type = os.path.basename(fname).split('.', maxsplit=1)
    return f'{file_dir}/{file_name}_conflated.{file_type}'
    

# %% ../nbs/42_entity-conflation.ipynb 43
def save_conflated_data(lbl_txt:List, lbl_file:str, trn_lbl:sp.csr_matrix, trn_file:str, 
                        tst_lbl:sp.csr_matrix, tst_file:str):
    lbl_file = get_conflated_path(lbl_file)
    trn_file = get_conflated_path(trn_file)
    tst_file = get_conflated_path(tst_file)

    save_raw_file(lbl_file, range(len(lbl_txt)), lbl_txt)
    sp.save_npz(trn_file, trn_lbl)
    sp.save_npz(tst_file, tst_lbl)
    

# %% ../nbs/42_entity-conflation.ipynb 46
def main(pred_file:str, trn_file:str, tst_file:str, lbl_file:str, embed_file:Optional[str]=None, 
         topk:Optional[int]=3, batch_size:Optional[int]=1024, min_thresh:Optional[int]=2, 
         max_thresh:Optional[int]=100, score_thresh:Optional[float]=25, freq_thresh:Optional[float]=50, 
         diff_thresh:Optional[float]=0.1, print_stats:Optional[bool]=False, encoding:Optional[str]='latin-1'):
    
    pred_lbl, trn_lbl, tst_lbl, (lbl_ids, lbl_txt), lbl_repr = load_data(pred_file, trn_file, tst_file, 
                                                                         lbl_file, embed_file, encoding=encoding)
    lbl_ids2txt = {k:v for k,v in zip(lbl_ids, lbl_txt)}

    data_lbl = Filter.topk(pred_lbl, k=topk)
    data_lbl = Filter.difference(data_lbl, t=diff_thresh)
    components = get_components(data_lbl, lbl_ids, lbl_repr=lbl_repr, score_thresh=score_thresh, 
                                freq_thresh=freq_thresh, batch_size=batch_size)

    valid_cluster_idxs = Filter.by_length(components, min_thresh=min_thresh, max_thresh=max_thresh)
    
    valid_components, lbl_ids2cluster_map = get_valid_components(components, valid_cluster_idxs)
    if print_stats: cluster_length_stats(valid_components)
    conflated_lbl_txt = get_conflated_info(valid_components, lbl_ids2txt)
    lbl_ids2cluster = get_id_to_cluster_idx_mapping(lbl_ids2cluster_map, lbl_ids)

    conflated_trn_lbl = get_conflated_matrix(trn_lbl, lbl_ids2cluster)
    conflated_tst_lbl = get_conflated_matrix(tst_lbl, lbl_ids2cluster, n_clusters=conflated_trn_lbl.shape[1])
    
    save_conflated_data(conflated_lbl_txt, lbl_file, conflated_trn_lbl, trn_file, conflated_tst_lbl, tst_file)
    

# %% ../nbs/42_entity-conflation.ipynb 48
def parse_args():
    parser = argparse.ArgumentParser()

    parser.add_argument('--pred_file', type=str, required=True)
    parser.add_argument('--lbl_file', type=str, required=True)
    parser.add_argument('--trn_file', type=str, required=True)
    parser.add_argument('--tst_file', type=str, required=True)
    parser.add_argument('--embed_file', type=str, default=None)

    parser.add_argument('--topk', type=int, default=3)
    parser.add_argument('--batch_size', type=int, default=1024)
    
    parser.add_argument('--min_thresh', type=int, default=2)
    parser.add_argument('--max_thresh', type=int, default=100)
    parser.add_argument('--score_thresh', type=float, default=25)
    parser.add_argument('--freq_thresh', type=float, default=50)
    parser.add_argument('--diff_thresh', type=float, default=0.1)

    parser.add_argument('--print_stats', action='store_true')
    parser.add_argument('--encoding', type=str, default='latin-1')
    
    return parser.parse_args()
    

# %% ../nbs/42_entity-conflation.ipynb 49
if __name__ == '__main__':
    args = parse_args()
    
    main(args.pred_file, args.trn_file, args.tst_file, args.lbl_file, args.embed_file, topk=args.topk, 
         batch_size=args.batch_size, min_thresh=args.min_thresh, max_thresh=args.max_thresh, 
         score_thresh=args.score_thresh, freq_thresh=args.freq_thresh, diff_thresh=args.diff_thresh, 
         print_stats=args.print_stats, encoding=args.encoding)

