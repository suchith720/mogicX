# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/25_sbert-for-msmarco-inference.ipynb.

# %% auto 0
__all__ = []

# %% ../nbs/25_sbert-for-msmarco-inference.ipynb 3
import os,torch,json, torch.multiprocessing as mp, joblib, numpy as np, scipy.sparse as sp
from sentence_transformers import SentenceTransformer, util
from tqdm import tqdm

from xcai.main import *
from xcai.metrics import mrr

# %% ../nbs/25_sbert-for-msmarco-inference.ipynb 5
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
os.environ['WANDB_PROJECT'] = 'mogicX_00-msmarco'

# %% ../nbs/25_sbert-for-msmarco-inference.ipynb 22
if __name__ == '__main__':
    config_file = '/scratch/scai/phd/aiz218323/datasets/msmarco/XC/configs/entity_gpt_exact.json'
    config_key = 'data_entity-gpt_exact'

    mname = 'sentence-transformers/msmarco-distilbert-dot-v5'

    topk = 200
    batch_size = 100
    device = 'cpu'

    input_args = parse_args()

    pkl_file = f'{input_args.pickle_dir}/mogicX/msmarco_data-entity-gpt_distilbert-base-uncased'
    pkl_file = f'{pkl_file}_sxc' if input_args.use_sxc_sampler else f'{pkl_file}_xcs'
    if input_args.only_test: pkl_file = f'{pkl_file}_only-test'
    pkl_file = f'{pkl_file}_exact'
    pkl_file = f'{pkl_file}.joblib'

    os.makedirs(os.path.dirname(pkl_file), exist_ok=True)
    block = build_block(pkl_file, config_file, input_args.use_sxc_sampler, config_key, do_build=input_args.build_block, 
                        only_test=input_args.only_test)

    model = SentenceTransformer(mname, device=device)

    queries = block.test.dset.data.data_info['input_text']
    labels = block.test.dset.data.lbl_info['input_text']

    lbl_embed = [model.encode(labels[idx:idx+batch_size], convert_to_tensor=True, device=device) for idx in tqdm(range(0, len(labels), batch_size))]
    lbl_embed = torch.cat(lbl_embed, dim=0)

    scores, idxs = [], []
    for idx in tqdm(range(0, len(queries), batch_size)):
        query_embed = model.encode(queries[idx:idx+batch_size], convert_to_tensor=True, device=device)
        sc = util.cos_sim(query_embed, lbl_embed)
        sc, idx = torch.topk(sc, k=topk, largest=True)
    
        scores.append(sc.to('cpu'))
        idxs.append(idx.to('cpu'))
    
    scores = torch.cat(scores, dim=0)
    idxs = torch.cat(idxs, dim=0)
    indptr = torch.arange(0, (scores.shape[0]+1) * topk, topk)

    pred_mat = sp.csr_matrix((scores.flatten(), idxs.flatten(), indptr.flatten()))
    print(mrr(pred_mat, block.test.dset.data.data_lbl, k=[10]))
    
