# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/42_entity-conflation.ipynb.

# %% auto 0
__all__ = ['show_conflated_labels', 'load_data', 'Filter', 'get_one_hop', 'normalize_matrix', 'compute_embed_similarity',
           'get_components', 'get_valid_components', 'get_conflated_info', 'get_id_to_cluster_idx_mapping',
           'get_conflated_matrix', 'cluster_length_stats', 'get_conflated_path', 'save_conflated_data', 'main',
           'parse_args']

# %% ../nbs/42_entity-conflation.ipynb 2
import scipy.sparse as sp, numpy as np, argparse, os, torch, pandas as pd
from tqdm.auto import tqdm
from termcolor import colored, COLORS
from scipy.sparse.csgraph import connected_components
from typing import List, Optional, Dict, Set, Tuple

from sugar.statistics_utils import matrix_stats
from sugar.core import load_raw_file, save_raw_file
from xclib.utils.sparse import retain_topk

# %% ../nbs/42_entity-conflation.ipynb 5
def show_conflated_labels(idxs:List, components:Dict, lbl_ids2txt:Dict, fname:Optional[str]=None):
    file = fname if fname is None else open(fname, 'w')
    for i, idx in enumerate(idxs):
        txt = " || ".join([lbl_ids2txt[o] for o in components[idx]])
        if fname is None: print(f'{i+1:03d}. {txt}')
        else: file.write(f'{i+1:03d}. {txt}\n')
    if fname is not None: file.close()
        

# %% ../nbs/42_entity-conflation.ipynb 9
def load_data(pred_file:str, trn_file:str, tst_file:str, lbl_info_file:str, embed_file:Optional[str]=None,
              encoding:Optional[str]='utf-8'):
    pred_lbl, trn_lbl, tst_lbl = sp.load_npz(pred_file), sp.load_npz(trn_file), sp.load_npz(tst_file)
    lbl_ids, lbl_txt = load_raw_file(lbl_info_file, encoding=encoding)
    lbl_repr = None if embed_file is None else torch.load(embed_file)
    return pred_lbl, trn_lbl, tst_lbl, (lbl_ids, lbl_txt), lbl_repr
    

# %% ../nbs/42_entity-conflation.ipynb 15
class Filter:

    @staticmethod
    def by_length(components:Dict, min_thresh:Optional[int]=1, max_thresh:Optional[int]=100):
        cluster_len = np.array([len(components[idx]) for idx in sorted(components)])

        mask = None if min_thresh is None else np.where(cluster_len >= min_thresh, 1, 0)
        if max_thresh is not None:
            max_mask = np.where(cluster_len <= max_thresh, 1, 0)
            mask = max_mask if mask is None else np.logical_and(mask, max_mask)

        return set() if mask is None else set(np.where(mask)[0]) 

    @staticmethod
    def topk(data_lbl:sp.csr_matrix, k:Optional[int]=3):
        return retain_topk(data_lbl, k=k)

    @staticmethod
    def threshold(data_lbl:sp.csr_matrix, t:int):
        idx = np.where(data_lbl.data < t)[0]
        data_lbl.data[idx] = 0
        data_lbl.eliminate_zeros()
        return data_lbl

    @staticmethod
    def difference(data_lbl:sp.csr_matrix, t:int):
        rowise_max = data_lbl.max(axis=1).toarray().ravel()
        scores = np.repeat(rowise_max, np.diff(data_lbl.indptr)) - data_lbl.data
        data_lbl.data[scores > t] = 0
        data_lbl.eliminate_zeros()
        return data_lbl
        

# %% ../nbs/42_entity-conflation.ipynb 17
def get_one_hop(data_lbl:sp.csr_matrix, batch_size:Optional[int]=1024):
    data_lbl = data_lbl.copy()
    data_lbl.data[:] = 1.0
    
    lbl_data = data_lbl.T.tocsr()
    lbl_lbl = [lbl_data[i:i+batch_size]@data_lbl for i in tqdm(range(0, lbl_data.shape[0], batch_size))]
    return sp.vstack(lbl_lbl)
    

# %% ../nbs/42_entity-conflation.ipynb 19
def normalize_matrix(lbl_lbl:sp.csr_matrix):
    lbl_lbl = lbl_lbl + sp.eye(lbl_lbl.shape[0])
    row_deg, col_deg = lbl_lbl.sum(axis=1), lbl_lbl.sum(axis=0)
    lbl_lbl = lbl_lbl.multiply(np.sqrt(1/row_deg)).multiply(np.sqrt(1/col_deg)).tocsr()
    return lbl_lbl
    

# %% ../nbs/42_entity-conflation.ipynb 20
def compute_embed_similarity(lbl_lbl:sp.csr_matrix, lbl_repr:torch.Tensor, batch_size:Optional[int]=1024):
    lbl_lbl = lbl_lbl.tocoo()
    scores = []
    for i in tqdm(range(0, lbl_lbl.nnz, batch_size)):
        row_idx, col_idx = lbl_lbl.row[i:i+batch_size], lbl_lbl.col[i:i+batch_size]
        sc = lbl_repr[row_idx].view(len(row_idx), 1, -1) @ lbl_repr[col_idx].view(len(col_idx), -1, 1)
        scores.append(sc.squeeze(1).squeeze(1))
    scores = torch.hstack(scores)
    lbl_lbl.data[:] = scores.numpy()
    return lbl_lbl.tocsr()
    

# %% ../nbs/42_entity-conflation.ipynb 21
def get_components(data_lbl:sp.csr_matrix, lbl_ids:List, lbl_repr:Optional[torch.Tensor]=None, 
                   score_thresh:Optional[float]=25, freq_thresh:Optional[float]=50, batch_size:Optional[int]=1024):
    lbl_lbl = get_one_hop(data_lbl, batch_size)
    lbl_lbl = normalize_matrix(lbl_lbl)
    lbl_lbl = Filter.threshold(lbl_lbl, t=np.percentile(lbl_lbl.data, q=freq_thresh))
    
    if lbl_repr is not None:
        lbl_lbl = compute_embed_similarity(lbl_lbl, lbl_repr, batch_size=batch_size)
        lbl_lbl = Filter.threshold(lbl_lbl, t=np.percentile(lbl_lbl.data, q=score_thresh))
    
    n_comp, clusters = connected_components(lbl_lbl, directed=False, return_labels=True)
    components = {}
    for idx,ids in zip(clusters, lbl_ids):
        components.setdefault(idx, []).append(ids)
    return components
    

# %% ../nbs/42_entity-conflation.ipynb 29
def get_valid_components(components:Dict, valid_cluster_idxs:Set):
    valid_components, lbl_ids2cluster = dict(), dict()

    curr_cluster_idx = 0
    for idx, cluster in components.items():
        if idx in valid_cluster_idxs:
            valid_components[curr_cluster_idx] = cluster
            for o in cluster: lbl_ids2cluster[o] = curr_cluster_idx
            curr_cluster_idx += 1
        else:
            for o in cluster:
                valid_components[curr_cluster_idx] = [o]
                lbl_ids2cluster[o] = curr_cluster_idx
                curr_cluster_idx += 1
                
    return valid_components, lbl_ids2cluster
    

# %% ../nbs/42_entity-conflation.ipynb 30
def _conflate_info_txt(txts:List, type:Optional[str]='max'):
    if type == "max":
        idx = int(np.argmax([len(o) for o in txts]))
        return txts[idx]
    elif type == "mid":
        idx = int(np.argsort([len(o) for o in txts])[(len(txts)-1)//2])
        return txts[idx]
    elif type == "concat":
        return " || ".join(txts)
    else:
        raise ValueError(f"Invalid type: {type}")
    
def get_conflated_info(components:Dict, lbl_ids2txt:Dict, type:Optional[str]="max"):
    return [_conflate_info_txt([lbl_ids2txt[o] for o in components[i]], type) for i in sorted(components)]
        

# %% ../nbs/42_entity-conflation.ipynb 31
def get_id_to_cluster_idx_mapping(lbl_ids2cluster_map:Dict, lbl_ids:List):
    return np.array([lbl_ids2cluster_map[o] for o in lbl_ids])
    

# %% ../nbs/42_entity-conflation.ipynb 32
def get_conflated_matrix(data_lbl:sp.csr_matrix, lbl_ids2cluster:Dict, n_clusters:Optional[Tuple]=None):
    indices = [lbl_ids2cluster[idx] for idx in data_lbl.indices]
    data = len(indices) * [1]

    if n_clusters is not None:
        assert max(indices) < n_clusters
    
    matrix = (
        sp.csr_matrix((data, indices, data_lbl.indptr), dtype=np.float32) 
        if n_clusters is None else 
        sp.csr_matrix((data, indices, data_lbl.indptr), shape=(data_lbl.shape[0], n_clusters), dtype=np.float32)
    )
    matrix.sum_duplicates()
    return matrix
    

# %% ../nbs/42_entity-conflation.ipynb 36
def cluster_length_stats(components):
    print(f'Number of components: {len(components)}')
    lengths = np.array([len(o) for o in components.values() if len(o) > 1])
    print(f'Number of clusters: {len(lengths)}', end='\n\n')
    with pd.option_context('display.precision', 3):
        print(pd.DataFrame(lengths).describe().T)

def _matrix_stats(mat, label='matrix'):
    print(label)
    stats = matrix_stats(mat)
    with pd.option_context('display.precision', 3, "display.max_columns", None):
        print(pd.DataFrame([stats]))

# %% ../nbs/42_entity-conflation.ipynb 44
def get_conflated_path(fname:str, output_dir:Optional[str]=None):
    file_dir = os.path.dirname(fname) if output_dir is None else output_dir
    os.makedirs(file_dir, exist_ok=True)

    file_name, file_type = os.path.basename(fname).split('.', maxsplit=1)
    return f'{file_dir}/{file_name}_conflated.{file_type}'
    

# %% ../nbs/42_entity-conflation.ipynb 45
def save_conflated_data(lbl_txt:List, lbl_file:str, trn_lbl:sp.csr_matrix, trn_file:str, tst_lbl:sp.csr_matrix, tst_file:str, 
        output_dir:Optional[str]=None):
    lbl_file = get_conflated_path(lbl_file, output_dir if output_dir is None else f"{output_dir}/raw_data")
    trn_file = get_conflated_path(trn_file, output_dir)
    tst_file = get_conflated_path(tst_file, output_dir)

    save_raw_file(lbl_file, range(len(lbl_txt)), lbl_txt)
    sp.save_npz(trn_file, trn_lbl)
    sp.save_npz(tst_file, tst_lbl)
    
def prune_pred_lbl(pred_lbl:sp.csr_matrix, topk:Optional[int]=None, diff_thresh:Optional[float]=None, pred_score_thresh:Optional[float]=None):
    # Pick top-k predictions
    data_lbl = pred_lbl if topk is None else Filter.topk(pred_lbl, k=topk) 

    # Difference based thresholding
    if diff_thresh is not None: data_lbl = Filter.difference(data_lbl, t=diff_thresh)

    # Score thresholding
    if pred_score_thresh is not None: data_lbl = Filter.threshold(data_lbl, t=pred_score_thresh)

    return data_lbl

def prune_clusters(components:Dict, min_size_thresh:Optional[int]=None, max_size_thresh:Optional[int]=None):
    valid_cluster_idxs = Filter.by_length(components, min_thresh=min_size_thresh, max_thresh=max_size_thresh)
    valid_components, lbl_ids2cluster_map = get_valid_components(components, valid_cluster_idxs)
    return valid_components, lbl_ids2cluster_map

# %% ../nbs/42_entity-conflation.ipynb 48
def main(pred_file:str, trn_file:str, tst_file:str, lbl_info_file:str, embed_file:Optional[str]=None, output_dir:Optional[str]=None,
        topk:Optional[int]=None, diff_thresh:Optional[float]=None, pred_score_thresh:Optional[float]=None, batch_size:Optional[int]=1024, 
        sim_score_thresh:Optional[float]=25, freq_thresh:Optional[float]=50, min_size_thresh:Optional[int]=None, max_size_thresh:Optional[int]=None, 
        print_stats:Optional[bool]=False, type:Optional[str]="max", encoding:Optional[str]='latin-1'):
    # Load data
    pred_lbl, trn_lbl, tst_lbl, lbl_info, lbl_repr = load_data(pred_file, trn_file, tst_file, lbl_info_file, embed_file, encoding=encoding)
    lbl_ids, lbl_txt = lbl_info

    # Prune predictions
    data_lbl = prune_pred_lbl(pred_lbl, topk, diff_thresh, pred_score_thresh)

    # Get connected components
    components = get_components(data_lbl, lbl_ids, lbl_repr=lbl_repr, score_thresh=sim_score_thresh, 
                                freq_thresh=freq_thresh, batch_size=batch_size)

    # Prune clusters
    components, lbl_ids2cluster_map = prune_clusters(components, min_size_thresh, max_size_thresh)

    # Statistics
    if print_stats: cluster_length_stats(components)

    # Conflate label text
    lbl_ids2txt = {k:v for k,v in zip(lbl_ids, lbl_txt)}
    conflated_lbl_txt = get_conflated_info(components, lbl_ids2txt, type)

    # Conflate matrix
    lbl_ids2cluster = get_id_to_cluster_idx_mapping(lbl_ids2cluster_map, lbl_ids)
    conflated_trn_lbl = get_conflated_matrix(trn_lbl, lbl_ids2cluster)
    conflated_tst_lbl = get_conflated_matrix(tst_lbl, lbl_ids2cluster, n_clusters=conflated_trn_lbl.shape[1])

    # Statistics
    if print_stats:
        _matrix_stats(conflated_trn_lbl, 'conflated_trn_lbl')
        _matrix_stats(conflated_tst_lbl, 'conflated_tst_lbl')
    
    save_conflated_data(conflated_lbl_txt, lbl_info_file, conflated_trn_lbl, trn_file, conflated_tst_lbl, tst_file, output_dir=output_dir)
    

# %% ../nbs/42_entity-conflation.ipynb 50
def parse_args():
    parser = argparse.ArgumentParser()

    parser.add_argument('--pred_file', type=str, required=True)
    parser.add_argument('--trn_file', type=str, required=True)
    parser.add_argument('--tst_file', type=str, required=True)
    parser.add_argument('--lbl_info_file', type=str, required=True)
    parser.add_argument('--embed_file', type=str, default=None)
    parser.add_argument('--output_dir', type=str, default=None)

    parser.add_argument('--topk', type=int, default=None)
    parser.add_argument('--diff_thresh', type=float, default=None)
    parser.add_argument('--pred_score_thresh', type=float, default=None)
    parser.add_argument('--batch_size', type=int, default=1024)
    
    parser.add_argument('--sim_score_thresh', type=float, default=25)
    parser.add_argument('--freq_thresh', type=float, default=50)

    parser.add_argument('--min_size_thresh', type=int, default=None)
    parser.add_argument('--max_size_thresh', type=int, default=None)

    parser.add_argument('--type', type=str, default='max')

    parser.add_argument('--print_stats', action='store_true')
    parser.add_argument('--encoding', type=str, default='latin-1')
    
    return parser.parse_args()
    

# %% ../nbs/42_entity-conflation.ipynb 51
if __name__ == '__main__':
    args = parse_args()
    
    main(args.pred_file, args.trn_file, args.tst_file, args.lbl_info_file, args.embed_file, output_dir=args.output_dir,
            topk=args.topk, diff_thresh=args.diff_thresh, pred_score_thresh=args.pred_score_thresh,  
            batch_size=args.batch_size, sim_score_thresh=args.sim_score_thresh, freq_thresh=args.freq_thresh, 
            min_size_thresh=args.min_size_thresh, max_size_thresh=args.max_size_thresh, print_stats=args.print_stats, 
            type=args.type, encoding=args.encoding)

