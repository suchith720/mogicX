# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/30_ngame-for-msmarco-with-hard-negatives.ipynb.

# %% auto 0
__all__ = ['DBT021']

# %% ../nbs/30_ngame-for-msmarco-with-hard-negatives.ipynb 3
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'

import torch,json, torch.multiprocessing as mp, joblib, numpy as np, scipy.sparse as sp, torch.nn as nn

from xcai.basics import *
from xcai.models.PPP0XX import DBT009,DBT011

# %% ../nbs/30_ngame-for-msmarco-with-hard-negatives.ipynb 5
os.environ['WANDB_PROJECT'] = 'mogicX_00-msmarco-04'

# %% ../nbs/30_ngame-for-msmarco-with-hard-negatives.ipynb 37
from typing import Optional
from xcai.losses import PKMMultiTripletFromScores
from xcai.models.PPP0XX import XCModelOutput

# %% ../nbs/30_ngame-for-msmarco-with-hard-negatives.ipynb 38
class DBT021(DBT009):

    def __init__(
        self,
        config,
        margin:Optional[float]=0.3,
        tau:Optional[float]=0.1,
        apply_softmax:Optional[bool]=True,
        n_negatives:Optional[int]=10,
        **kwargs
    ):
        super().__init__(config, margin=margin, tau=tau, apply_softmax=apply_softmax, n_negatives=n_negatives, **kwargs)
        self.loss_fn = PKMMultiTripletFromScores(margin=margin, n_negatives=n_negatives, tau=tau, apply_softmax=apply_softmax, 
                                                 reduce='mean')

    def _get_scores(self, data_repr:torch.Tensor, lbl2data_repr:torch.Tensor, neg2data_repr:Optional[torch.Tensor]=None):
        lbl_scores = data_repr @ lbl2data_repr.T

        neg_scores = None
        if neg2data_repr is not None:
            bsz = data_repr.shape[0]
            n_meta = neg2data_repr.shape[0] // bsz

            neg_scores = data_repr.unsqueeze(1) @ neg2data_repr.view(bsz, n_meta, -1).transpose(1, 2)
            neg_scores = neg_scores.squeeze(1)
        
        return lbl_scores if neg_scores is None else torch.hstack([lbl_scores, neg_scores])

    def _get_indices(self, lbl2data_idx:torch.Tensor, neg2data_idx:Optional[torch.Tensor]=None):
        bsz = len(lbl2data_idx)
        
        lbl_idx = torch.repeat_interleave(lbl2data_idx.unsqueeze(0), bsz, 0)

        neg_idx = None
        if neg2data_idx is not None:
            n_meta = len(neg2data_idx) // bsz
            neg_idx = neg2data_idx.view(bsz, n_meta)
        
        return lbl_idx if neg_idx is None else torch.hstack([lbl_idx, neg_idx])
    
    def forward(
        self,
        data_input_ids:Optional[torch.Tensor]=None,
        data_attention_mask:Optional[torch.Tensor]=None,
        lbl2data_data2ptr:Optional[torch.Tensor]=None,
        lbl2data_idx:Optional[torch.Tensor]=None,
        lbl2data_input_ids:Optional[torch.Tensor]=None,
        lbl2data_attention_mask:Optional[torch.Tensor]=None,
        plbl2data_data2ptr:Optional[torch.Tensor]=None,
        plbl2data_idx:Optional[torch.Tensor]=None,

        neg2data_data2ptr:Optional[torch.Tensor]=None,
        neg2data_idx:Optional[torch.Tensor]=None,
        neg2data_input_ids:Optional[torch.Tensor]=None,
        neg2data_attention_mask:Optional[torch.Tensor]=None,
        
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs
    ):
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        
        if self.use_encoder_parallel: 
            encoder = nn.DataParallel(module=self.encoder)
        else: encoder = self.encoder
        
        data_o, data_repr = encoder(data_input_ids, data_attention_mask, 
                                    output_attentions=output_attentions, 
                                    output_hidden_states=output_hidden_states,
                                    return_dict=return_dict)
        
        loss, lbl2data_repr = None, None
        if lbl2data_input_ids is not None:
            lbl2data_o, lbl2data_repr = encoder(lbl2data_input_ids, lbl2data_attention_mask,  
                                                output_attentions=output_attentions, 
                                                output_hidden_states=output_hidden_states,
                                                return_dict=return_dict)
            neg2data_repr = None
            if neg2data_input_ids is not None:
                neg2data_o, neg2data_repr = encoder(neg2data_input_ids, neg2data_attention_mask,
                                                    output_attentions=output_attentions, 
                                                    output_hidden_states=output_hidden_states,
                                                    return_dict=return_dict)

                assert torch.all(neg2data_data2ptr == neg2data_data2ptr.max()), f'All datapoints should have equal negatives'
                
            scores, idx = self._get_scores(data_repr, lbl2data_repr, neg2data_repr), self._get_indices(lbl2data_idx, neg2data_idx)
            loss = self.loss_fn(scores, idx, plbl2data_data2ptr, plbl2data_idx)

        if not return_dict:
            o = (data_repr, lbl2data_repr)
            return ((loss,) + o) if loss is not None else o

        return XCModelOutput(
            loss=loss,
            data_repr=data_repr,
            lbl2data_repr=lbl2data_repr,
        )
        

# %% ../nbs/30_ngame-for-msmarco-with-hard-negatives.ipynb 54
if __name__ == '__main__':
    output_dir = '/home/aiscuser/scratch1/outputs/mogicX/30_ngame-for-msmarco-with-hard-negatives-001'

    config_file = '/data/datasets/msmarco/XC/configs/negatives_exact.json'
    config_key = 'data_negatives_exact'
    
    mname = 'sentence-transformers/msmarco-distilbert-cos-v5'

    input_args = parse_args()

    pkl_file = f'{input_args.pickle_dir}/mogicX/msmarco_data-neg_distilbert-base-uncased'
    pkl_file = f'{pkl_file}_sxc' if input_args.use_sxc_sampler else f'{pkl_file}_xcs'
    if input_args.only_test: pkl_file = f'{pkl_file}_only-test'
    pkl_file = f'{pkl_file}.joblib'

    do_inference = input_args.do_train_inference or input_args.do_test_inference or input_args.save_train_prediction or input_args.save_test_prediction or input_args.save_representation

    os.makedirs(os.path.dirname(pkl_file), exist_ok=True)
    block = build_block(pkl_file, config_file, input_args.use_sxc_sampler, config_key, do_build=input_args.build_block, 
                        only_test=input_args.only_test, use_meta_distribution=False, meta_oversample=True, n_sdata_meta_samples=200)

    args = XCLearningArguments(
        output_dir=output_dir,
        logging_first_step=True,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=800,
        representation_num_beams=200,
        representation_accumulation_steps=10,
        save_strategy="steps",
        eval_strategy="steps",
        eval_steps=500,
        save_steps=500,
        save_total_limit=5,
        num_train_epochs=30, # 300,
        predict_with_representation=True,
        representation_search_type='BRUTEFORCE',
        adam_epsilon=1e-6,
        warmup_steps=100,
        weight_decay=0.01,
        learning_rate=2e-7,
    
        group_by_cluster=True,
        num_clustering_warmup_epochs=10,
        num_cluster_update_epochs=5,
        num_cluster_size_update_epochs=25,
        clustering_type='EXPO',
        minimum_cluster_size=2,
        maximum_cluster_size=1600,
    
        metric_for_best_model='P@1',
        load_best_model_at_end=True,
        target_indices_key='plbl2data_idx',
        target_pointer_key='plbl2data_data2ptr',
    
        use_encoder_parallel=True,
        max_grad_norm=None,
        fp16=True,
    )

    def model_fn(mname, bsz):
        model = DBT021.from_pretrained(mname, bsz=bsz, tn_targ=5000, margin=0.1, tau=1.0, n_negatives=100, 
                                       apply_softmax=True, use_encoder_parallel=True)
        return model
    
    def init_fn(model): 
        model.init_dr_head()

    metric = PrecReclMrr(block.test.dset.n_lbl, block.test.data_lbl_filterer,
                     pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200], mk=[5, 10, 20])

    bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()

    model = load_model(args.output_dir, model_fn, {"mname": mname, "bsz": bsz}, init_fn, do_inference=do_inference, use_pretrained=input_args.use_pretrained)
    
    learn = XCLearner(
        model=model,
        args=args,
        train_dataset=block.train.dset,
        eval_dataset=block.test.dset,
        data_collator=block.collator,
        compute_metrics=metric,
    )
    
    main(learn, input_args, n_lbl=block.test.dset.n_lbl)
    
