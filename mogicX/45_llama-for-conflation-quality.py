# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/45_llama-for-conflation-quality.ipynb.

# %% auto 0
__all__ = ['PROMPT_TEMPLATE', 'make_prompts', 'evaluate_clusters_in_batch', 'parse_args']

# %% ../nbs/45_llama-for-conflation-quality.ipynb 1
import torch, json, argparse, re
from transformers import AutoTokenizer, AutoModelForCausalLM

from tqdm.auto import tqdm

from sugar.core import load_raw_file, save_raw_file

# %% ../nbs/45_llama-for-conflation-quality.ipynb 5
PROMPT_TEMPLATE = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Cutting Knowledge Date: December 2023
Today Date: 23 July 2024

You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>

You are an expert evaluator of entity conflation.

Task:
Determine whether the given entities belong to a coherent cluster (i.e., they represent the same underlying entity).

Instructions:
1. Provide your answer **only** in valid JSON format.
2. The JSON must contain exactly two keys:
   - "score": an integer from 1 (very incoherent) to 5 (highly coherent).
   - "reason": a short explanation for the score.
3. Ensure that the output can be parsed by:
   ```python
   import json
   content = json.loads(output)
   ```
where `content` has exactly two keys: "score" and "reason".

Example:
Input: Apple iPhone 14 Pro || Samsung Galaxy S23 || Google Pixel 7
Output:
{{
"score": 1,
"reason": "Entities belong to the same category (smartphones) but are from different manufacturers and product lines."
}}

Now evaluate the following cluster:
{cluster}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""

# %% ../nbs/45_llama-for-conflation-quality.ipynb 7
def make_prompts(clusters):
    return [PROMPT_TEMPLATE.format(cluster=o) for o in clusters]
    

# %% ../nbs/45_llama-for-conflation-quality.ipynb 8
def evaluate_clusters_in_batch(clusters, batch_size=4, max_new_tokens=1024):
    prompts = make_prompts(clusters)
    
    inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(model.device)
    
    responses, generations = [], []
    for i in tqdm(range(0, len(prompts), batch_size)):
        batch_input_ids = inputs["input_ids"][i:i+batch_size]
        batch_attention_mask = inputs["attention_mask"][i:i+batch_size]
    
        outputs = model.generate(
            input_ids=batch_input_ids,
            attention_mask=batch_attention_mask,
            max_new_tokens=max_new_tokens,
            do_sample=False
        )
    
        batch_texts = tokenizer.batch_decode(outputs[:, batch_input_ids.shape[1]:], skip_special_tokens=True)
        for text in batch_texts:
            generations.append(text)
            match = re.search(r"\{.*?\}", text + "}", re.DOTALL)
            try:
                content = json.loads(match.group(0))
            except:
                content = {}
            responses.append(content)
    
    return responses, generations
    

# %% ../nbs/45_llama-for-conflation-quality.ipynb 9
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--input_file', type=str, required=True)
    parser.add_argument('--output_file', type=str, required=True)
    parser.add_argument('--batch_size', type=int, default=4)
    return parser.parse_known_args()[0]
    

# %% ../nbs/45_llama-for-conflation-quality.ipynb 10
if __name__ == '__main__':
    args = parse_args()
    
    model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto"
    )

    ids, txts = load_raw_file(args.input_file)
    responses, generations = evaluate_clusters_in_batch(txts, batch_size=args.batch_size)
    
    for i,g,t in zip(ids, responses, txts): 
        g['identifier'] = i; g['entities'] = t.split(" || ")

    with open(args.output_file, 'w') as file:
        json.dump(responses, file, indent=4)

    save_raw_file(f'{args.output_file}.csv', ids, generations)
        
