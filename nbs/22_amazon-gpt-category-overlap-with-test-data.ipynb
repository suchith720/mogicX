{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd50c2f8-f246-4a47-9735-bb52f5ef9b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp 22_amazon-gpt-category-overlap-with-test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e529a48b-0884-4dff-a137-f8531e503393",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2bd8334-4c93-4e19-b618-412fbe916fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F, math, scipy.sparse as sp, os, numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from xclib.utils.sparse import retain_topk\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ab6fbf8-7036-4362-b7a3-233ed6f98b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcai.main import *\n",
    "from xcai.analysis import *\n",
    "from xcai.data import XCDataset\n",
    "from xcai.analysis import *\n",
    "from xcai.models.modeling_utils import Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8885796d-d707-41ab-b76a-40a652819c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, BatchEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90687048-bd7c-4c6d-b740-943498d902ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sugar.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c41a61-e75c-4d28-897a-3e86619ec013",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac4dfcfc-143f-4962-9104-3a6d4f648156",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_info_file = '/home/scai/phd/aiz218323/scratch/datasets/benchmarks/(mapped)LF-AmazonTitles-1.3M//raw_data/test.raw.txt'\n",
    "meta_info_file = '/home/scai/phd/aiz218323/scratch/datasets/benchmarks/LF-AmazonTitles-1.3M_gpt-conflations/raw_data/category_gpt_conflated-1.raw.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48b90a06-6493-4b1c-bf92-82cc7c93a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_info = load_raw_file(test_info_file)[1]\n",
    "meta_info = load_raw_file(meta_info_file)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c9caaa1-0a37-4ce0-a2c7-8260d9c5336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "tokz = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5df243ce-146b-4c12-803e-f0e5ec346492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_tokens = tokz(test_info, return_tensors='pt', padding=True, truncation=True)\n",
    "meta_tokens = tokz(meta_info, return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe83165c-1d1d-453f-8a90-134695d5f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataloaderLite(Dataset):\n",
    "\n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.num_data = len(data['input_ids'])\n",
    "        self.current_position = 0\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def next_data(self):\n",
    "        batch = BatchEncoding({k:v[self.current_position:self.current_position+self.batch_size] for k,v in self.data.items()})\n",
    "        self.current_position += self.batch_size\n",
    "        \n",
    "        if self.current_position > self.num_data:\n",
    "            self.current_position = 0\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(self.num_data/self.batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe80db79-706b-4194-b99f-aee9b074174c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "933a5193-cc3f-4061-972c-6411d1a8daea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataloaderLite(test_tokens, batch_size=100)\n",
    "meta_dataloader = DataloaderLite(meta_tokens, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e26dba2-9581-4dca-b477-42dd0b7d0ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11b2db0c-91dc-4b2e-afe0-9b125392b85c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                          | 2/9703 [00:02<3:59:34,  1.48s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 122.19 MiB is free. Process 24832 has 708.00 MiB memory in use. Including non-PyTorch memory, this process has 30.92 GiB memory in use. Of the allocated memory 30.48 GiB is allocated by PyTorch, and 79.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m batch \u001b[38;5;241m=\u001b[39m test_dataloader\u001b[38;5;241m.\u001b[39mnext_data()\n\u001b[1;32m      4\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m----> 5\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mrepr\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(Pooling\u001b[38;5;241m.\u001b[39mmean_pooling(output[\u001b[38;5;241m0\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m test_repr\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mrepr\u001b[39m\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[0;32m~/scratch/anaconda3/envs/mogic/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scratch/anaconda3/envs/mogic/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/scratch/anaconda3/envs/mogic/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:796\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[1;32m    792\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_attention_mask_for_sdpa(\n\u001b[1;32m    793\u001b[0m             attention_mask, embeddings\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    794\u001b[0m         )\n\u001b[0;32m--> 796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scratch/anaconda3/envs/mogic/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scratch/anaconda3/envs/mogic/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/scratch/anaconda3/envs/mogic/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:549\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    541\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    542\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    543\u001b[0m         hidden_state,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m         output_attentions,\n\u001b[1;32m    547\u001b[0m     )\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 549\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/scratch/anaconda3/envs/mogic/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scratch/anaconda3/envs/mogic/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/scratch/anaconda3/envs/mogic/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:490\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msa_output must be a tuple but it is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(sa_output)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    489\u001b[0m     sa_output \u001b[38;5;241m=\u001b[39m sa_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 490\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa_layer_norm(\u001b[43msa_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# Feed Forward Network\u001b[39;00m\n\u001b[1;32m    493\u001b[0m ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(sa_output)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 122.19 MiB is free. Process 24832 has 708.00 MiB memory in use. Including non-PyTorch memory, this process has 30.92 GiB memory in use. Of the allocated memory 30.48 GiB is allocated by PyTorch, and 79.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "test_repr = []\n",
    "for i in tqdm(range(len(test_dataloader))):\n",
    "    batch = test_dataloader.next_data()\n",
    "    batch = batch.to(model.device)\n",
    "    output = model(**batch)\n",
    "    repr = F.normalize(Pooling.mean_pooling(output[0], batch['attention_mask']), dim=1)\n",
    "    test_repr.append(repr.cpu())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b364276f-e838-435e-98a6-e9059a1a2df5",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fefd9810-b5f8-4d22-9555-89aa8ba18a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = '/scratch/scai/phd/aiz218323/datasets/processed/'\n",
    "pkl_file = f'{pkl_dir}/mogicX/amazontitles_data-gpt-category-conflated-1_distilbert-base-uncased_sxc.joblib'\n",
    "\n",
    "config_file = '../configs/16_ngame-linker-for-amazontitles-003_gpt-category-conflated-1.json'\n",
    "config_key = 'data_category'\n",
    "\n",
    "block = build_block(pkl_file, config_file, use_sxc=True, config_key=config_key)\n",
    "linker_block = block.linker_dset('cat_meta', remove_empty=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72b9801d-58e9-4896-80df-37545a5ff6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dir = '/home/scai/phd/aiz218323/scratch/outputs/mogicX/16_ngame-linker-for-amazontitles-004/predictions/'\n",
    "pred_lbl = sp.load_npz(f'{pred_dir}/test_predictions_zs.npz')\n",
    "\n",
    "pred_block = get_pred_dset(retain_topk(pred_lbl, k=10), linker_block.test.dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6d02579-51a3-4e97-9f50-d81f3a035c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = TextDataset(pred_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06eb6226-de38-4530-9233-c304ac91d9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/home/scai/phd/aiz218323/scratch/outputs/mogicX/16_ngame-linker-for-amazontitles-004/examples'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da93b8c-8045-4dd7-bdea-50d9e9f61f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e5c1e23-6f3c-4d15-83b5-55f983e4d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.random.permutation(pred_lbl.shape[0])[:1000]\n",
    "dset.dump(f'{save_dir}/zero_shot.txt', idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c426e48a-ba17-4e15-b3a5-389879bf0d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[7m\u001b[36mdata_input_text\u001b[0m \u001b[36m: New York City Transit Buses 1945-1975 Photo Archive\u001b[0m\n",
      "\u001b[5m\u001b[7m\u001b[91mlbl2data_input_text\u001b[0m \u001b[91m: ['Urban Transportation History Book', 'New York City History Book', 'Public Transportation Book', 'Transportation Memorabilia', 'Vintage Transportation Documentary', 'Public Transportation', 'Urban Transit Merchandise', 'Toy Bus', 'Rail Transit Books', 'Urban Transport Book']\u001b[0m\n",
      "\n",
      "\u001b[5m\u001b[7m\u001b[36mdata_input_text\u001b[0m \u001b[36m: AC/DC - Family Jewels\u001b[0m\n",
      "\u001b[5m\u001b[7m\u001b[91mlbl2data_input_text\u001b[0m \u001b[91m: ['Family & Genealogy', 'Family Biography', 'Gifts for Family Members', 'Family Movie Collection', 'Family DVD', \"Children's Gifts & Decorations\", 'Family Clothing', 'DC Motor', 'Family Photo & Albums', 'DC Collectibles']\u001b[0m\n",
      "\n",
      "\u001b[5m\u001b[7m\u001b[36mdata_input_text\u001b[0m \u001b[36m: NBA New Orleans Hornets Wool Blend Adjustable Snapback Hat, One Size,  Blue\u001b[0m\n",
      "\u001b[5m\u001b[7m\u001b[91mlbl2data_input_text\u001b[0m \u001b[91m: ['Bowling Shoes', 'Wide Brim Hat', \"Men's Basketball Shoes\", 'Rain Boots', 'Jazz Shoes', \"Kids' Basketball Shoes\", \"Boys' Basketball Shoes\", 'Sports Hat', 'Custom Hats', \"Men's Sports Hats\"]\u001b[0m\n",
      "\n",
      "\u001b[5m\u001b[7m\u001b[36mdata_input_text\u001b[0m \u001b[36m: ECCO Men's BIOM Hybrid Golf Shoe\u001b[0m\n",
      "\u001b[5m\u001b[7m\u001b[91mlbl2data_input_text\u001b[0m \u001b[91m: [\"Men's Tennis Shoes\", \"Men's Bowling Shoes\", \"Men's Golf Shoes\", 'Golf Shoe', \"Men's Cycling Shoe\", \"Men's Motocross Boots\", \"Men's Outdoor Shoes\", \"Men's Indoor Soccer Shoes\", \"Men's Cross Training Shoes\", \"Men's Tennis Shoe\"]\u001b[0m\n",
      "\n",
      "\u001b[5m\u001b[7m\u001b[36mdata_input_text\u001b[0m \u001b[36m: The Orchard: A Memoir\u001b[0m\n",
      "\u001b[5m\u001b[7m\u001b[91mlbl2data_input_text\u001b[0m \u001b[91m: ['Memoir', 'Autobiographical Memoir', 'Farming Memoir', 'Amish Memoir Book', 'Gardening Memoir', 'Agricultural Memoir', 'Farm Memoir', 'Agricultural Memoir Book', 'Memoir/Cookbook/Gardening', 'Memoir and Cookbook']\u001b[0m\n",
      "\n",
      "\u001b[5m\u001b[7m\u001b[36mdata_input_text\u001b[0m \u001b[36m: Strawberry 100%, Vol. 10\u001b[0m\n",
      "\u001b[5m\u001b[7m\u001b[91mlbl2data_input_text\u001b[0m \u001b[91m: ['Wine and Viticulture Book', 'Vermiculture Book', 'Agricultural Memoir', 'Agricultural Memoir Book', 'Christmas CD', 'Bluegrass Christmas Music CD', 'Christmas Music CD Collection', 'Agricultural Magazine', 'Polka Music CD', 'Agricultural Marketing Book']\u001b[0m\n",
      "\n",
      "\u001b[5m\u001b[7m\u001b[36mdata_input_text\u001b[0m \u001b[36m: Taylor Made RBZ Hybrid Headcover (RocketBallz Rescue Golf Club Cover) NEW\u001b[0m\n",
      "\u001b[5m\u001b[7m\u001b[91mlbl2data_input_text\u001b[0m \u001b[91m: ['Rocketry Accessories', 'Model Rocket Accessories', 'Model Rocketry Accessories', 'Rocket Accessories', 'Hobby Rocket Kits', 'Golf Headcover', 'RC Toy Accessories', 'RC Quadcopter Kits', 'RC Rock Crawling Accessories', 'RC Quadcopter Accessories']\u001b[0m\n",
      "\n",
      "\u001b[5m\u001b[7m\u001b[36mdata_input_text\u001b[0m \u001b[36m: The Magic Hour\u001b[0m\n",
      "\u001b[5m\u001b[7m\u001b[91mlbl2data_input_text\u001b[0m \u001b[91m: ['Magic Show Video', 'Magic Show Performance', 'Magic Performance', 'Magic/Illusion TV Special', 'Magic Tricks & Entertainment', 'Magic Show', 'Magic & Illusion', 'Hour Meter', 'Magic/Illusion Entertainment', 'Magic Performance Video']\u001b[0m\n",
      "\n",
      "\u001b[5m\u001b[7m\u001b[36mdata_input_text\u001b[0m \u001b[36m: MLB Boston Red Sox Twin Bedding Set with Pillow Sham\u001b[0m\n",
      "\u001b[5m\u001b[7m\u001b[91mlbl2data_input_text\u001b[0m \u001b[91m: ['Bedding & Pillow', 'Bedding Pillow', 'Bed Pillow', 'Bed Sheets & Pillowcases', 'Bedding Sheets & Pillowcases', 'Bedding and Pillow', 'Bedding Sheets and Pillowcases', 'Twin Nursing and Support Pillows', 'Bedding and Pillow Covers', 'Bedding/Pillow']\u001b[0m\n",
      "\n",
      "\u001b[5m\u001b[7m\u001b[36mdata_input_text\u001b[0m \u001b[36m: Sparkling Cyanide [VHS]\u001b[0m\n",
      "\u001b[5m\u001b[7m\u001b[91mlbl2data_input_text\u001b[0m \u001b[91m: ['VHS', 'VHS Film', 'VHS Media', 'VHS Cassettes', 'Home Decorating VHS', 'VHS Collectibles', 'VHS Entertainment', 'Film (VHS)', 'VHS Animation', 'VHS Video']\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37803635-8e66-4d5f-a84d-24407dae28cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
